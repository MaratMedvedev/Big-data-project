{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a31b3c0-49fc-41b7-bbe6-ac4377efe04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "team = 28\n",
    "\n",
    "warehouse = \"project/hive/warehouse\"\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "        .appName(\"{} - spark ML\".format(team))\\\n",
    "        .master(\"yarn\") \\\n",
    "        .config(\"hive.metastore.uris\", \"thrift://hadoop-02.uni.innopolis.ru:9883\")\\\n",
    "        .config(\"spark.sql.warehouse.dir\", warehouse)\\\n",
    "        .config(\"spark.sql.avro.compression.codec\", \"snappy\")\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b827df-7402-43fa-b0fc-b85ea0d692fb",
   "metadata": {},
   "source": [
    "# Hive tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6eb908c3-f1e0-4609-880f-f1d872b3a527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Table(name='property_details_part_buck', database='team28_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q1_results', database='team28_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q2_results', database='team28_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q3_results', database='team28_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q4_results', database='team28_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='q5_results', database='team28_projectdb', description=None, tableType='EXTERNAL', isTemporary=False), Table(name='target_price_prediction_fixed', database='team28_projectdb', description=None, tableType='EXTERNAL', isTemporary=False)]\n"
     ]
    }
   ],
   "source": [
    "print(spark.catalog.listTables(\"team28_projectdb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9e5d7a8-a0c8-403f-a765-fe5215ebbc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "property_details = spark.read.format(\"avro\").table('team28_projectdb.property_details_part_buck')\n",
    "q1_results = spark.read.format(\"avro\").table('team28_projectdb.q1_results')\n",
    "q2_results = spark.read.format(\"avro\").table('team28_projectdb.q2_results')\n",
    "q3_results = spark.read.format(\"avro\").table('team28_projectdb.q3_results')\n",
    "q4_results = spark.read.format(\"avro\").table('team28_projectdb.q4_results')\n",
    "q5_results = spark.read.format(\"avro\").table('team28_projectdb.q5_results')\n",
    "target_price_prediction = spark.read.format(\"avro\").table('team28_projectdb.target_price_prediction_fixed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff68613d-4fe6-44c9-809b-48b5a51ff376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- under_construction: boolean (nullable = true)\n",
      " |-- rera: boolean (nullable = true)\n",
      " |-- bhk_no: integer (nullable = true)\n",
      " |-- bhk_or_rk: string (nullable = true)\n",
      " |-- square_ft: decimal(20,10) (nullable = true)\n",
      " |-- ready_to_move: boolean (nullable = true)\n",
      " |-- resale: boolean (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- longitude: decimal(19,10) (nullable = true)\n",
      " |-- latitude: decimal(19,10) (nullable = true)\n",
      " |-- posted_by: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- under_construction: boolean (nullable = true)\n",
      " |-- mean_price: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- rera: boolean (nullable = true)\n",
      " |-- mean_price: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- bhk_no: integer (nullable = true)\n",
      " |-- mean_price: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- resale: boolean (nullable = true)\n",
      " |-- mean_price: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- longitude: decimal(19,10) (nullable = true)\n",
      " |-- latitude: decimal(19,10) (nullable = true)\n",
      " |-- price: float (nullable = true)\n",
      "\n",
      "root\n",
      " |-- property_id: integer (nullable = true)\n",
      " |-- price_in_lacs: decimal(10,1) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "property_details.printSchema()\n",
    "q1_results.printSchema()\n",
    "q2_results.printSchema()\n",
    "q3_results.printSchema()\n",
    "q4_results.printSchema()\n",
    "q5_results.printSchema()\n",
    "target_price_prediction.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9447d9b4-82b1-418c-ad5c-ab4284d7aff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----+------+---------+-----------------+-------------+------+--------------------+------------+-------------+---------+\n",
      "|   id|under_construction| rera|bhk_no|bhk_or_rk|        square_ft|ready_to_move|resale|             address|   longitude|     latitude|posted_by|\n",
      "+-----+------------------+-----+------+---------+-----------------+-------------+------+--------------------+------------+-------------+---------+\n",
      "|29389|              true|false|     3|      BHK|   979.2909793000|        false| false|\"\"\"Barasat_Kolkat...|2.7476390000|88.6009020000|  Builder|\n",
      "|29247|              true| true|     3|      BHK|  1471.8750000000|        false| false|\"\"\"Gajuwaka_Visak...|7.0000000000|83.2167000000|  Builder|\n",
      "|29002|              true| true|     3|      BHK|157454.5455000000|        false| false|\"\"\"8th Phase JP N...|2.9053600000|77.5804030000|  Builder|\n",
      "+-----+------------------+-----+------+---------+-----------------+-------------+------+--------------------+------------+-------------+---------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "property_details.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc94b8e0-fa4e-45f0-a212-fd1624856dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|under_construction|mean_price|\n",
      "+------------------+----------+\n",
      "|             false| 125.86329|\n",
      "|              true| 220.63313|\n",
      "+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q1_results.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0953aedc-b34c-49c2-bbe0-d51ada5c6b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+\n",
      "| rera|mean_price|\n",
      "+-----+----------+\n",
      "|false| 112.56696|\n",
      "| true|  207.9746|\n",
      "+-----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q2_results.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "79a5c386-01e8-4740-b8cd-20c55aada47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|bhk_no|mean_price|\n",
      "+------+----------+\n",
      "|     1|  72.56343|\n",
      "|     2|  97.17659|\n",
      "+------+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q3_results.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f8405bb-f8a6-4bb7-b75d-aaf4c8342144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|resale|mean_price|\n",
      "+------+----------+\n",
      "| false|  637.8141|\n",
      "|  true| 105.40544|\n",
      "+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q4_results.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f8178c6-4b70-48a2-bb46-0c4d3ab28263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----+\n",
      "|   longitude|     latitude|price|\n",
      "+------------+-------------+-----+\n",
      "|8.4854800000|73.8037730000| 70.0|\n",
      "|5.4510000000|81.8280000000| 95.0|\n",
      "+------------+-------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "q5_results.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97d6bd8a-a14e-45aa-8cca-da8440d8aff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+\n",
      "|property_id|price_in_lacs|\n",
      "+-----------+-------------+\n",
      "|      29445|         40.0|\n",
      "|      29433|         50.0|\n",
      "|      29428|         24.0|\n",
      "+-----------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "target_price_prediction.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b64abc-2445-4cf4-b62b-dcb8c4cb7763",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c946ff6d-c037-4410-92a8-bf360c3cab89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------------+-----+------+---------+--------------+-------------+------+--------------------+------------+-------------+---------+-----------+-------------+\n",
      "|   id|under_construction| rera|bhk_no|bhk_or_rk|     square_ft|ready_to_move|resale|             address|   longitude|     latitude|posted_by|property_id|price_in_lacs|\n",
      "+-----+------------------+-----+------+---------+--------------+-------------+------+--------------------+------------+-------------+---------+-----------+-------------+\n",
      "|29389|              true|false|     3|      BHK|979.2909793000|        false| false|\"\"\"Barasat_Kolkat...|2.7476390000|88.6009020000|  Builder|      29389|         27.9|\n",
      "+-----+------------------+-----+------+---------+--------------+-------------+------+--------------------+------------+-------------+---------+-----------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = property_details.join(target_price_prediction, property_details.id == target_price_prediction.property_id, \"inner\")\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0e8dab9b-fa8f-42ff-bcf9-70ec83eb370d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+------+---------+--------------+-------------+------+--------------------+------------+-------------+---------+-------------+\n",
      "|under_construction| rera|bhk_no|bhk_or_rk|     square_ft|ready_to_move|resale|             address|   longitude|     latitude|posted_by|price_in_lacs|\n",
      "+------------------+-----+------+---------+--------------+-------------+------+--------------------+------------+-------------+---------+-------------+\n",
      "|              true|false|     3|      BHK|979.2909793000|        false| false|\"\"\"Barasat_Kolkat...|2.7476390000|88.6009020000|  Builder|         27.9|\n",
      "+------------------+-----+------+---------+--------------+-------------+------+--------------------+------------+-------------+---------+-------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.drop(\"id\")\n",
    "df = df.drop(\"property_id\")\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aae890c1-b9fa-429c-b3a7-d70cea0f4169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, lit, udf\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "import math\n",
    "\n",
    "class EcefCoordinatesTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_cols = Param(Params._dummy(), \"input_cols\", \"input columns names.\", typeConverter=TypeConverters.toString)\n",
    "    output_cols = Param(Params._dummy(), \"output_cols\", \"output columns names.\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, input_cols: str = \"input\", output_cols: str = \"output\"):\n",
    "        super(EcefCoordinatesTransformer, self).__init__()\n",
    "        self._setDefault(input_cols=None, output_cols=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def set_params(self, input_cols: str = \"input\", output_cols: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def get_input_cols(self):\n",
    "        return self.getOrDefault(self.input_cols)\n",
    "    \n",
    "    def get_output_cols(self):\n",
    "        return self.getOrDefault(self.output_cols)\n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_cols = self.get_input_cols().split(',')\n",
    "        output_cols = self.get_output_cols().split(',')\n",
    "        assert len(input_cols) == 2, \"Expected two input columns: latitude and longitude.\"\n",
    "        assert len(output_cols) == 2, \"Expected two output columns: x and y.\"\n",
    "\n",
    "        def ecef_transform(longitude, latitude):\n",
    "            a = 6378137.0  # semi-major axis of the WGS-84 ellipsoid in meters\n",
    "            e = 0.081819190842622  # eccentricity of the WGS-84 ellipsoid\n",
    "\n",
    "            longitude_rad = F.radians(longitude)\n",
    "            latitude_rad = F.radians(latitude)\n",
    "\n",
    "            # calculate the radius of curvature in the prime vertical\n",
    "            N = a / (1 - e**2 * latitude_rad**2)**0.5\n",
    "\n",
    "            x = N * F.cos(latitude_rad) * F.cos(longitude_rad)\n",
    "            y = N * F.cos(latitude_rad) * F.sin(longitude_rad)\n",
    "\n",
    "            return x, y\n",
    "        \n",
    "        x, y = ecef_transform(df[input_cols[0]], df[input_cols[1]])\n",
    "        df = df.withColumn(output_cols[0], x)\n",
    "        df = df.withColumn(output_cols[1], y)  \n",
    "        df = df.drop(input_cols[0])\n",
    "        df = df.drop(input_cols[1])\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd4d7264-e4a7-45e2-b850-a9aa4d4cbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCols, HasOutputCols, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "class MultiBooleanToIntTransformer(Transformer, HasInputCols, HasOutputCols, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_cols = Param(Params._dummy(), \"input_cols\", \"input columns names.\", typeConverter=TypeConverters.toString)\n",
    "    output_cols = Param(Params._dummy(), \"output_cols\", \"output columns names.\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, input_cols: str = \"input\", output_cols: str = \"output\"):\n",
    "        super(MultiBooleanToIntTransformer, self).__init__()\n",
    "        self._setDefault(input_cols=None, output_cols=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def set_params(self, input_cols: str = \"input\", output_cols: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def get_input_cols(self):\n",
    "        return self.getOrDefault(self.input_cols)\n",
    "    \n",
    "    def get_output_cols(self):\n",
    "        return self.getOrDefault(self.output_cols)\n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_cols = self.get_input_cols().split(',')\n",
    "        output_cols = self.get_output_cols().split(',')\n",
    "        \n",
    "        assert len(input_cols) == len(output_cols), \"The number of input and output columns must match.\"\n",
    "        \n",
    "        # apply the transformation to each input column and create corresponding output columns\n",
    "        for input_col, output_col in zip(input_cols, output_cols):\n",
    "            transform_logic = when(df[input_col] == True, 1).otherwise(0)\n",
    "            df = df.withColumn(output_col, transform_logic)\n",
    "        \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ae222b1-b9ff-479d-9577-061188847940",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param, Params, TypeConverters\n",
    "from pyspark.ml.util import DefaultParamsReadable, DefaultParamsWritable\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "class AddressTransformer(Transformer, HasInputCol, HasOutputCol, DefaultParamsReadable, DefaultParamsWritable):\n",
    "    input_cols = Param(Params._dummy(), \"input_cols\", \"input columns names.\", typeConverter=TypeConverters.toString)\n",
    "    output_cols = Param(Params._dummy(), \"output_cols\", \"output columns names.\", typeConverter=TypeConverters.toString)\n",
    "    \n",
    "    @keyword_only\n",
    "    def __init__(self, input_cols: str = \"input\", output_cols: str = \"output\"):\n",
    "        super(AddressTransformer, self).__init__()\n",
    "        self._setDefault(input_cols=None, output_cols=None)\n",
    "        kwargs = self._input_kwargs\n",
    "        self.set_params(**kwargs)\n",
    "        \n",
    "    @keyword_only\n",
    "    def set_params(self, input_cols: str = \"input\", output_cols: str = \"output\"):\n",
    "        kwargs = self._input_kwargs\n",
    "        self._set(**kwargs)\n",
    "    \n",
    "    def get_input_cols(self):\n",
    "        return self.getOrDefault(self.input_cols)\n",
    "    \n",
    "    def get_output_cols(self):\n",
    "        return self.getOrDefault(self.output_cols)\n",
    "    \n",
    "    def _transform(self, df: DataFrame):\n",
    "        input_cols = self.get_input_cols().split(',')\n",
    "        output_cols = self.get_output_cols().split(',')\n",
    "        assert len(input_cols) == 1, \"Expected one input column: address.\"\n",
    "        assert len(output_cols) == 1, \"Expected one output column: city.\"\n",
    "\n",
    "        # define the UDF for extracting city from address\n",
    "        extract_city_udf = F.udf(lambda address: address.split(\",\")[-1].strip(), StringType())\n",
    "\n",
    "        df = df.withColumn(output_cols[0], extract_city_udf(df[input_cols[0]]))\n",
    "        df = df.drop(input_cols[0])\n",
    "    \n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c851318-dd24-4328-abdd-0e2f9a2648f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.sql.types import StringType, StructType, StructField\n",
    "\n",
    "ecef = EcefCoordinatesTransformer(input_cols=\"longitude,latitude\", output_cols=\"x,y\")\n",
    "boolean_to_int = MultiBooleanToIntTransformer(input_cols=\"under_construction,rera,ready_to_move,resale\", output_cols=\"under_construction,rera,ready_to_move,resale\")\n",
    "address = AddressTransformer(input_cols=\"address\", output_cols=\"city\")\n",
    "\n",
    "indexer_bhk_or_rk = StringIndexer(inputCol=\"bhk_or_rk\", outputCol=\"indexed_bhk_or_rk\")\n",
    "indexer_city = StringIndexer(inputCol=\"city\", outputCol=\"indexed_city\")\n",
    "indexer_posted_by = StringIndexer(inputCol=\"posted_by\", outputCol=\"indexed_posted_by\")\n",
    "\n",
    "one_hot_bhk_or_rk = OneHotEncoder(inputCol=\"indexed_bhk_or_rk\", outputCol=\"bhk_or_rk_enc\")\n",
    "one_hot_city = OneHotEncoder(inputCol=\"indexed_city\", outputCol=\"city_enc\")\n",
    "one_hot_posted_by = OneHotEncoder(inputCol=\"indexed_posted_by\", outputCol=\"posted_by_enc\")\n",
    "\n",
    "assembler = VectorAssembler(inputCols=[\"under_construction\", \"rera\", \"bhk_no\", \"square_ft\", \"ready_to_move\", \"resale\", \"x\", \"y\", \"bhk_or_rk_enc\", \"city_enc\", \"posted_by_enc\"], outputCol=\"features\")\n",
    "\n",
    "pipeline = Pipeline(stages=[ecef, boolean_to_int, address, indexer_bhk_or_rk, indexer_city, indexer_posted_by, one_hot_bhk_or_rk, one_hot_city, one_hot_posted_by, assembler])\n",
    "model = pipeline.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1ba20cc5-2b7c-4da9-b3b6-907d5e39fa0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+------+---------+-----------------+-------------+------+---------+-------------+-------------------+------------------+--------------------+-----------------+------------+-----------------+-------------+-------------------+-------------+--------------------+\n",
      "|under_construction|rera|bhk_no|bhk_or_rk|        square_ft|ready_to_move|resale|posted_by|price_in_lacs|                  x|                 y|                city|indexed_bhk_or_rk|indexed_city|indexed_posted_by|bhk_or_rk_enc|           city_enc|posted_by_enc|            features|\n",
      "+------------------+----+------+---------+-----------------+-------------+------+---------+-------------+-------------------+------------------+--------------------+-----------------+------------+-----------------+-------------+-------------------+-------------+--------------------+\n",
      "|                 1|   0|     3|      BHK|   979.2909793000|            0|     0|  Builder|         27.9|  156812.5685895108| 7525.771101390836|\"\"\"Barasat_Kolkat...|              0.0|       186.0|              2.0|(1,[0],[1.0])| (6898,[186],[1.0])|    (2,[],[])|(6909,[0,2,3,6,7,...|\n",
      "|                 1|   1|     3|      BHK|  1471.8750000000|            0|     0|  Builder|         47.1|  753071.5994163619| 92465.56566278606|\"\"\"Gajuwaka_Visak...|              0.0|       326.0|              2.0|(1,[0],[1.0])| (6898,[326],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 1|   1|     3|      BHK|157454.5455000000|            0|     0|  Builder|       8660.0| 1378464.1041684377| 69959.26545151592|\"\"\"8th Phase JP N...|              0.0|       263.0|              2.0|(1,[0],[1.0])| (6898,[263],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 1|   1|     3|      BHK|  1128.0000000000|            0|     0|  Builder|         28.2| 515964.36375713366|35795.062624192586|\"\"\"Hurhuru_Hazari...|              0.0|      1735.0|              2.0|(1,[0],[1.0])|(6898,[1735],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 0|   1|     4|      BHK|  1228.2776820000|            1|     0|  Builder|        270.0| 1871402.3441996581|293053.78651934094|\"\"\"Sopan Baug_Mah...|              0.0|      6231.0|              2.0|(1,[0],[1.0])|(6898,[6231],[1.0])|    (2,[],[])|(6909,[1,2,3,4,6,...|\n",
      "|                 1|   1|     1|      BHK|   318.6521959000|            0|     0|  Builder|         50.5| 1864403.9363443153| 303853.6174949534|\"\"\"Dahisar (East)...|              0.0|       421.0|              2.0|(1,[0],[1.0])| (6898,[421],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 1|   1|     2|      BHK|   886.8421053000|            0|     0|  Builder|         33.7| 1392507.1436971955| 68542.26544562717|\"\"\"Seegehalli_Ban...|              0.0|       875.0|              2.0|(1,[0],[1.0])| (6898,[875],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 0|   0|     1|      BHK|   422.1533695000|            1|     0|  Builder|         10.9| 1363479.1728781718| 64300.04554131183|\"\"\"Hoshangabad Ro...|              0.0|       386.0|              2.0|(1,[0],[1.0])| (6898,[386],[1.0])|    (2,[],[])|(6909,[2,3,4,6,7,...|\n",
      "|                 0|   0|     3|      BHK|   982.9763866000|            1|     0|  Builder|         35.8| 1084018.5968200294| 57949.36973365241|\"\"\"Pudupakkam Vil...|              0.0|       993.0|              2.0|(1,[0],[1.0])| (6898,[993],[1.0])|    (2,[],[])|(6909,[2,3,4,6,7,...|\n",
      "|                 1|   1|     3|      BHK|  1298.8505750000|            0|     0|  Builder|         56.5| 1377426.6759300253| 68777.93832517086|\"\"\"Gottigere_Bang...|              0.0|       492.0|              2.0|(1,[0],[1.0])| (6898,[492],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 1|   1|     2|      BHK|  1000.0000000000|            0|     0|  Builder|         28.5|  1558184.548583494|186694.60074013757|\"\"\"Jagatpura_Jaip...|              0.0|         5.0|              2.0|(1,[0],[1.0])|   (6898,[5],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 1|   1|     2|      BHK|  2413.9844620000|            0|     0|  Builder|         43.5| 1401334.3314626517| 210284.7879751169| \"\"\"Diva_Lalitpur\"\"\"|              0.0|      2293.0|              2.0|(1,[0],[1.0])|(6898,[2293],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 0|   0|     2|      BHK|  1138.8888890000|            1|     0|  Builder|         41.0|  931360.8177119881|20332.936323254118|\"\"\"Shankar Nagar_...|              0.0|      1220.0|              2.0|(1,[0],[1.0])|(6898,[1220],[1.0])|    (2,[],[])|(6909,[2,3,4,6,7,...|\n",
      "|                 0|   0|     3|      BHK|   753.8461538000|            1|     0|  Builder|          4.9| 1859339.7718026852|293259.78505253885| \"\"\"Uran_Lalitpur\"\"\"|              0.0|      6495.0|              2.0|(1,[0],[1.0])|(6898,[6495],[1.0])|    (2,[],[])|(6909,[2,3,4,6,7,...|\n",
      "|                 0|   0|     3|      BHK|  1664.9269310000|            1|     0|  Builder|         31.9| 1125509.4682569725|62184.661184317185|\"\"\"Ganga Nagar_Ja...|              0.0|      4012.0|              2.0|(1,[0],[1.0])|(6898,[4012],[1.0])|    (2,[],[])|(6909,[2,3,4,6,7,...|\n",
      "|                 1|   0|     2|      BHK|  1003.4482760000|            0|     0|  Builder|         29.1|  367735.9837041591| 28925.49847673483|\"\"\"Bilasi Town_De...|              0.0|      3562.0|              2.0|(1,[0],[1.0])|(6898,[3562],[1.0])|    (2,[],[])|(6909,[0,2,3,6,7,...|\n",
      "|                 1|   1|     3|      BHK|  1733.3333330000|            0|     0|  Builder|         78.0| 1486734.8258829927|23433.361506651392|\"\"\"Uliyakovil_Kol...|              0.0|      6474.0|              2.0|(1,[0],[1.0])|(6898,[6474],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 1|   1|     3|      BHK|  1202.8571430000|            0|     0|  Builder|         42.1|-186444.70691238582|-20105.79999655662|\"\"\"Jalukbari_Guwa...|              0.0|       461.0|              2.0|(1,[0],[1.0])| (6898,[461],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 1|   1|     1|       RK|   366.5776195000|            0|     0|  Builder|         49.4| 1284232.4381264455|105363.79146770267|\"\"\"Khanda Colony_...|              1.0|      2482.0|              2.0|    (1,[],[])|(6898,[2482],[1.0])|    (2,[],[])|(6909,[0,1,2,3,6,...|\n",
      "|                 0|   0|     1|      BHK|   592.8705441000|            1|     0|  Builder|         47.4| 1092200.3499029002| 58898.33203611563|\"\"\"Anna Nagar Wes...|              0.0|       632.0|              2.0|(1,[0],[1.0])| (6898,[632],[1.0])|    (2,[],[])|(6909,[2,3,4,6,7,...|\n",
      "+------------------+----+------+---------+-----------------+-------------+------+---------+-------------+-------------------+------------------+--------------------+-----------------+------------+-----------------+-------------+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2769c0a-b610-4b22-be0e-861b89e52738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(6909,[0,2,3,6,7,...|  27.9|\n",
      "|(6909,[0,1,2,3,6,...|  47.1|\n",
      "|(6909,[0,1,2,3,6,...|8660.0|\n",
      "|(6909,[0,1,2,3,6,...|  28.2|\n",
      "|(6909,[1,2,3,4,6,...| 270.0|\n",
      "|(6909,[0,1,2,3,6,...|  50.5|\n",
      "|(6909,[0,1,2,3,6,...|  33.7|\n",
      "|(6909,[2,3,4,6,7,...|  10.9|\n",
      "|(6909,[2,3,4,6,7,...|  35.8|\n",
      "|(6909,[0,1,2,3,6,...|  56.5|\n",
      "|(6909,[0,1,2,3,6,...|  28.5|\n",
      "|(6909,[0,1,2,3,6,...|  43.5|\n",
      "|(6909,[2,3,4,6,7,...|  41.0|\n",
      "|(6909,[2,3,4,6,7,...|   4.9|\n",
      "|(6909,[2,3,4,6,7,...|  31.9|\n",
      "|(6909,[0,2,3,6,7,...|  29.1|\n",
      "|(6909,[0,1,2,3,6,...|  78.0|\n",
      "|(6909,[0,1,2,3,6,...|  42.1|\n",
      "|(6909,[0,1,2,3,6,...|  49.4|\n",
      "|(6909,[2,3,4,6,7,...|  47.4|\n",
      "+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.select([\"features\", \"price_in_lacs\"])\n",
    "df = df.withColumnRenamed(\"price_in_lacs\", \"label\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "631f8e5c-50b9-49bc-bae3-2a6e2fd2656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed = 121)\n",
    "\n",
    "def run(command):\n",
    "    return os.popen(command).read()\n",
    "\n",
    "train_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/train\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/data/train/*.json > data/train.json\")\n",
    "\n",
    "test_data.select(\"features\", \"label\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"json\")\\\n",
    "    .save(\"project/data/test\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/data/test/*.json > data/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11766c25-ced8-4d35-9cf4-be378bb44c18",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "18b9a962-232d-439b-9875-d090ea032380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+\n",
      "|            features|label|         prediction|\n",
      "+--------------------+-----+-------------------+\n",
      "|(6909,[0,1,2,3,6,...| 32.0| 177.14943896963865|\n",
      "|(6909,[0,1,2,3,6,...| 21.0| 411.19415551132204|\n",
      "|(6909,[0,1,2,3,6,...| 50.5| 232.19121670413136|\n",
      "|(6909,[0,1,2,3,6,...| 33.0|  233.2392884737082|\n",
      "|(6909,[0,1,2,3,6,...| 28.2|  310.5818947866019|\n",
      "|(6909,[0,1,2,3,6,...| 72.0| 357.35280786163105|\n",
      "|(6909,[0,1,2,3,6,...|  7.8|-141.02149704186877|\n",
      "|(6909,[0,1,2,3,6,...| 39.7|  283.1172003655636|\n",
      "|(6909,[0,1,2,3,6,...|  7.4| 194.94245283733102|\n",
      "|(6909,[0,2,3,6,7,...| 27.9| 261.69806073766847|\n",
      "|(6909,[0,2,3,6,7,...| 29.1| 239.07553998173248|\n",
      "|(6909,[1,2,3,4,6,...| 46.3|  393.3035141306177|\n",
      "|(6909,[1,2,3,4,6,...| 63.4| 187.60478392191067|\n",
      "|(6909,[1,2,3,4,6,...| 78.5| 261.50303959607356|\n",
      "|(6909,[1,2,3,4,6,...| 60.0| 370.75666532532136|\n",
      "|(6909,[2,3,4,6,7,...|560.0| 312.43859509249603|\n",
      "|(6909,[2,3,4,6,7,...| 57.0|  55.50484137373516|\n",
      "|(6909,[2,3,4,6,7,...| 62.4| 356.54960052050455|\n",
      "|(6909,[0,1,2,3,6,...| 63.1|  301.7922103269091|\n",
      "|(6909,[0,1,2,3,6,...| 41.0| 311.74224922299044|\n",
      "+--------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr = LinearRegression()\n",
    "model_lr = lr.fit(train_data)\n",
    "\n",
    "predictions_lr = model_lr.transform(test_data)\n",
    "predictions_lr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d87c7ab4-8ea3-430f-9120-efc916b6a661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 518.7776786995653\n",
      "R2: -0.0071604587004237\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# evaluate the performance of the model\n",
    "evaluator = RegressionEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse_lr = evaluator.evaluate(predictions_lr)\n",
    "r2_lr = evaluator.evaluate(predictions_lr, {evaluator.metricName: \"r2\"})\n",
    "print(f\"RMSE: {rmse_lr}\")\n",
    "print(f\"R2: {r2_lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "be1f3d88-763b-402a-97e9-a1f9fa9a2a8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegressionModel: uid=LinearRegression_0172c5a25f44, numFeatures=6909"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "grid = ParamGridBuilder()\n",
    "grid = grid.addGrid(\n",
    "                    model_lr.aggregationDepth, [2, 3, 4])\\\n",
    "                    .addGrid(model_lr.regParam, np.logspace(1e-3,1e-1)\n",
    "                    )\\\n",
    "                    .build()\n",
    "\n",
    "cv = CrossValidator(estimator = lr, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = evaluator,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8c61d48-1aa7-48e2-96d9-b555063bd11a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Param(parent='LinearRegression_0172c5a25f44', name='maxIter', doc='max number of iterations (>= 0).'): 100,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='regParam', doc='regularization parameter (>= 0).'): 0.0,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='epsilon', doc='The shape parameter to control the amount of robustness. Must be > 1.0. Only valid when loss is huber'): 1.35,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='predictionCol', doc='prediction column name.'): 'prediction',\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='featuresCol', doc='features column name.'): 'features',\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='fitIntercept', doc='whether to fit an intercept term.'): True,\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='labelCol', doc='label column name.'): 'label',\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='solver', doc='The solver algorithm for optimization. Supported options: auto, normal, l-bfgs.'): 'auto',\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='loss', doc='The loss function to be optimized. Supported options: squaredError, huber.'): 'squaredError',\n",
      " Param(parent='LinearRegression_0172c5a25f44', name='standardization', doc='whether to standardize the training features before fitting the model.'): True}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "model_lr_best = bestModel\n",
    "pprint(model_lr_best.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "22878940-095b-4258-97c9-824eb98626a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr_best.write().overwrite().save(\"project/models/model_lr_best\")\n",
    "\n",
    "run(\"hdfs dfs -get project/models/model_lr models/model_lr_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bd2ed73a-eb1f-4594-8537-465cf90272fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+-------------------+\n",
      "|            features|label|         prediction|\n",
      "+--------------------+-----+-------------------+\n",
      "|(6909,[0,1,2,3,6,...| 32.0| 177.14943864793614|\n",
      "|(6909,[0,1,2,3,6,...| 21.0|  411.1941551787495|\n",
      "|(6909,[0,1,2,3,6,...| 50.5|  232.1912165576932|\n",
      "|(6909,[0,1,2,3,6,...| 33.0| 233.23928761464342|\n",
      "|(6909,[0,1,2,3,6,...| 28.2|  310.5818938199013|\n",
      "|(6909,[0,1,2,3,6,...| 72.0| 357.35280738288986|\n",
      "|(6909,[0,1,2,3,6,...|  7.8|-141.02149661822065|\n",
      "|(6909,[0,1,2,3,6,...| 39.7|  283.1172070808016|\n",
      "|(6909,[0,1,2,3,6,...|  7.4|  194.9424610563676|\n",
      "|(6909,[0,2,3,6,7,...| 27.9|  261.6980594569151|\n",
      "|(6909,[0,2,3,6,7,...| 29.1| 239.07554899133504|\n",
      "|(6909,[1,2,3,4,6,...| 46.3| 393.30351342757695|\n",
      "|(6909,[1,2,3,4,6,...| 63.4|  187.6047832666457|\n",
      "|(6909,[1,2,3,4,6,...| 78.5| 261.50303954625895|\n",
      "|(6909,[1,2,3,4,6,...| 60.0|  370.7566758539425|\n",
      "|(6909,[2,3,4,6,7,...|560.0|  312.4385941513352|\n",
      "|(6909,[2,3,4,6,7,...| 57.0|  55.50484051656903|\n",
      "|(6909,[2,3,4,6,7,...| 62.4|  356.5495999716188|\n",
      "|(6909,[0,1,2,3,6,...| 63.1|  301.7922095145185|\n",
      "|(6909,[0,1,2,3,6,...| 41.0|  311.7422482227803|\n",
      "+--------------------+-----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_lr_best_predictions = model_lr_best.transform(test_data)\n",
    "model_lr_best_predictions.show()\n",
    "\n",
    "model_lr_best_predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model_lr_best_predictions.csv\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/output/model_lr_best_predictions.csv/*.csv > output/model_lr_best_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3b909b23-ed4a-4c8a-987e-0aebbe999ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 518.7776787241265\n",
      "R2: -0.007160458795790969\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# evaluate the performance of the model\n",
    "rmse_lr_best = evaluator.evaluate(model_lr_best_predictions)\n",
    "r2_lr_best = evaluator.evaluate(model_lr_best_predictions, {evaluator.metricName: \"r2\"})\n",
    "print(f\"RMSE: {rmse_lr_best}\")\n",
    "print(f\"R2: {r2_lr_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbb5e7e-0dc8-46e6-afea-80adaf8c9f2d",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "62563f0a-4cce-4cfd-bbd5-95a1b4000f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+------------------+\n",
      "|            features|label|        prediction|\n",
      "+--------------------+-----+------------------+\n",
      "|(6909,[0,1,2,3,6,...| 32.0|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...| 21.0|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...| 50.5|112.85182170542636|\n",
      "|(6909,[0,1,2,3,6,...| 33.0|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...| 28.2|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...| 72.0|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...|  7.8|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...| 39.7|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...|  7.4|56.550205434062306|\n",
      "|(6909,[0,2,3,6,7,...| 27.9|56.550205434062306|\n",
      "|(6909,[0,2,3,6,7,...| 29.1|56.550205434062306|\n",
      "|(6909,[1,2,3,4,6,...| 46.3|56.550205434062306|\n",
      "|(6909,[1,2,3,4,6,...| 63.4|56.550205434062306|\n",
      "|(6909,[1,2,3,4,6,...| 78.5| 183.6871064467766|\n",
      "|(6909,[1,2,3,4,6,...| 60.0| 349.7930047694754|\n",
      "|(6909,[2,3,4,6,7,...|560.0| 349.7930047694754|\n",
      "|(6909,[2,3,4,6,7,...| 57.0|56.550205434062306|\n",
      "|(6909,[2,3,4,6,7,...| 62.4|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...| 63.1|56.550205434062306|\n",
      "|(6909,[0,1,2,3,6,...| 41.0|56.550205434062306|\n",
      "+--------------------+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# linear regression\n",
    "from pyspark.ml.regression import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor()\n",
    "model_dt = dt.fit(train_data)\n",
    "\n",
    "predictions_dt = model_dt.transform(test_data)\n",
    "predictions_dt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b5a3f8d7-2315-4881-a735-a56cc6311654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 157.3401059333119\n",
      "R2: 0.9073564943850804\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# evaluate the performance of the model\n",
    "rmse_dt = evaluator.evaluate(predictions_dt)\n",
    "r2_dt = evaluator.evaluate(predictions_dt, {evaluator.metricName: \"r2\"})\n",
    "print(f\"RMSE: {rmse_dt}\")\n",
    "print(f\"R2: {r2_dt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "591c56b6-6b53-4b60-ae1a-b308a0321ab8",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    719\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-37fe8fe1da91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     numFolds=2)\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mcvModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0mbestModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcvModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbestModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mbestModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             raise TypeError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/pyspark/ml/tuning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0minheritable_thread_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 _parallelFitTasks(est, train, eva, validation, epm, collectSubModelsParam))\n\u001b[0;32m--> 689\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubModel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap_unordered\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m                 \u001b[0mmetrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnFolds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mcollectSubModelsParam\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    722\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator \n",
    "import numpy as np\n",
    "\n",
    "grid = ParamGridBuilder() \\\n",
    "   .addGrid(dt.maxDepth, [3, 5, 7]) \\\n",
    "   .addGrid(dt.minInstancesPerNode, [2, 5, 8]) \\\n",
    "   .build()\n",
    "\n",
    "cv = CrossValidator(estimator = dt, \n",
    "                    estimatorParamMaps = grid, \n",
    "                    evaluator = evaluator,\n",
    "                    parallelism = 5,\n",
    "                    numFolds=3)\n",
    "\n",
    "cvModel = cv.fit(train_data)\n",
    "bestModel = cvModel.bestModel\n",
    "bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d81083-cd59-49aa-a2d6-5c29e9369b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "model_dt_best = bestModel\n",
    "pprint(model_dt_best.extractParamMap())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1729ffb3-0618-497b-99a3-5110f9c16d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_best.write().overwrite().save(\"project/models/model_dt_best\")\n",
    "\n",
    "run(\"hdfs dfs -get project/models/model_dt models/model_dt_best\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef1288a9-e7c0-4934-aa37-f326f15d001b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dt_best_predictions = model_dt_best.transform(test_data)\n",
    "model_dt_best_predictions.show()\n",
    "\n",
    "model_dt_best_predictions.select(\"label\", \"prediction\")\\\n",
    "    .coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/model_dt_best_predictions.csv\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/output/model_dt_best_predictions.csv/*.csv > output/model_dt_best_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dfef516-b07c-4f1d-b974-a8040a23285a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator \n",
    "\n",
    "# evaluate the performance of the model\n",
    "rmse_dt_best = evaluator.evaluate(model_dt_best_predictions)\n",
    "r2_dt_best = evaluator.evaluate(model_dt_best_predictions, {evaluator.metricName: \"r2\"})\n",
    "print(f\"RMSE: {rmse_dt_best}\")\n",
    "print(f\"R2: {r2_dt_best}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0958fe-dca8-4163-924f-3a21aa7634d1",
   "metadata": {},
   "source": [
    "# Compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5af96d-4829-4e0e-8482-6da6a5072b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data frame to report performance of the models\n",
    "models = [[str(model_lr_best), rmse_lr_best, r2_lr_best], [str(model_dt_best), rmse_dt_best, r2_dt_best]]\n",
    "\n",
    "df = spark.createDataFrame(models, [\"model\", \"RMSE\", \"R2\"])\n",
    "df.show(truncate=False)\n",
    "\n",
    "# save it to HDFS\n",
    "df.coalesce(1)\\\n",
    "    .write\\\n",
    "    .mode(\"overwrite\")\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"sep\", \",\")\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .save(\"project/output/evaluation.csv\")\n",
    "\n",
    "run(\"hdfs dfs -cat project/output/evaluation.csv/*.csv > output/evaluation.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
